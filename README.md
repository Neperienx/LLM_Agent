# LLM Agent Platform (PoC)

This repository hosts a local-first agent orchestration playground. The proof of concept now ships
as a pure Python tool that runs entirely from the command line. It provides:

- a Python runtime capable of loading JSON pipeline definitions, rendering Jinja prompts, and
  running simple `llm_call`, `transform`, and `store` steps without any web server or frontend

The initial sample pipeline (`pipelines/book_outline.json`) turns high-level book inputs into a
markdown outline and chapter plan using a deterministic local LLM stub.

## Getting Started

### 1. Python environment (Python 3.13)

```bash
python3.13 -m venv .venv
source .venv/bin/activate
pip install -e .
```

### 2. Use the interactive launcher

Run the helper script to either spin up a brand-new project or execute an existing
pipeline definition without remembering any CLI arguments:

```bash
python run.py
```

#### Create a new project flow

1. Choose **Create a new project** when the menu appears.
2. Provide a project identifier (used for filenames), display name, and optional description.
3. Define any input fields your pipeline should collect (press Enter on an empty name to finish).
4. Enter the Jinja prompt template (type `END` on its own line when finished). If you skip this step a helpful default prompt is used.
5. The script writes a JSON pipeline under `pipelines/<identifier>.json` and a template under `templates/<identifier>.j2`.
6. Confirm whether you want to immediately run the new project and provide values for each input if you do.

#### Open an existing project flow

1. Choose **Open an existing project** to see a numbered list of pipelines found in `pipelines/`.
2. Select the project by number (or type a custom path) and supply the requested input values.
3. The runner executes the pipeline and prints the run ID plus the artifact directory location.

### 3. Run pipelines from the CLI

Once the environment is active you can still interact with pipelines using the bundled
`agent` command:

```bash
agent list
agent run book_outline \
  -p title="Voyagers" \
  -p genre="Sci-Fi" \
  -p audience="Adult"
```

Each run writes artifacts to `artifacts/run-<timestamp>/` and includes a `run.json` manifest plus any
persisted files generated by `store` steps.

### 4. Visualise prompts and data flow

Launch the new Tkinter dashboard to inspect how prompts, transforms, and storage steps
connect inside each pipeline:

```bash
agent visualize
```

Select a pipeline from the sidebar to see its declared inputs, step dependencies, and the
contents of each prompt template.

## Repository Layout

```
core/          # pipeline orchestrator, prompt service, CLI
pipelines/     # JSON pipeline definitions
templates/     # Prompt templates rendered during llm_call steps
tools/         # Python transform helpers referenced by pipelines
artifacts/     # Run outputs (created at runtime)
```

## Next Steps

- Expand the orchestrator with branching/foreach semantics and richer artifact metadata.
- Replace the stub LLM client with pluggable backends (OpenAI, local ggml, etc.).
- Surface per-step outputs/logs in the CLI and allow rerunning from intermediate steps.
