# LLM Agent Platform (PoC)

This repository hosts a local-first agent orchestration playground. The current Proof of Concept
provides:

- a Python runtime capable of loading JSON pipeline definitions, rendering Jinja prompts, and
  running simple `llm_call`, `transform`, and `store` steps
- a FastAPI backend to list pipelines and trigger runs
- a Vite + React GUI that surfaces the available pipelines and lets you run them end-to-end

The initial sample pipeline (`pipelines/book_outline.json`) turns high-level book inputs into a
markdown outline and chapter plan using a deterministic local LLM stub.

## Getting Started

### 1. Python environment

```bash
uv venv        # or python -m venv .venv
source .venv/bin/activate
uv pip install -r <(uv pip compile pyproject.toml)  # or pip install -e .
```

> If you do not have [`uv`](https://github.com/astral-sh/uv) installed yet, you can run `pip install -e .`
> instead of the `uv` commands above.

### 2. Launch the API server

```bash
uvicorn server.main:app --reload
```

The server exposes:

- `GET /health` — health probe
- `GET /pipelines` — enumerate available JSON pipelines
- `POST /runs` — trigger a pipeline run with JSON body `{ "pipeline": "book_outline", "inputs": {...} }`

Artifacts are written to `artifacts/run-<timestamp>/` and include a `run.json` manifest and any
persisted files generated by `store` steps.

### 3. Run from the CLI (optional)

```bash
agent list
agent run book_outline \
  -p title="Voyagers" \
  -p genre="Sci-Fi" \
  -p audience="Adult"
```

### 4. Launch the GUI

```bash
cd ui
npm install
npm run dev
```

The Vite dev server (http://localhost:5173) proxies API calls to `http://localhost:8000`. Use the
interface to:

1. Select the "Book Outline Draft" pipeline.
2. Fill in the title/genre/audience fields.
3. Run the pipeline and inspect the generated artifacts path.

## Repository Layout

```
core/          # pipeline orchestrator, prompt service, CLI
server/        # FastAPI application
ui/            # React + Vite front-end PoC
pipelines/     # JSON pipeline definitions
templates/     # Prompt templates rendered during llm_call steps
tools/         # Python transform helpers referenced by pipelines
artifacts/     # Run outputs (created at runtime)
memory/        # Reserved for future persistence features
```

## Next Steps

- Expand the orchestrator with branching/foreach semantics and richer artifact metadata.
- Replace the stub LLM client with pluggable backends (OpenAI, local ggml, etc.).
- Surface per-step outputs/logs in the GUI and allow rerunning from intermediate steps.
