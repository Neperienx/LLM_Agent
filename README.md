# LLM Agent Platform (PoC)

This repository hosts a local-first agent orchestration playground. The proof of concept now ships
as a pure Python tool that runs entirely from the command line. It provides:

- a Python runtime capable of loading JSON pipeline definitions, rendering Jinja prompts, and
  running simple `llm_call`, `transform`, and `store` steps without any web server or frontend

The initial sample pipeline (`pipelines/book_outline.json`) turns high-level book inputs into a
markdown outline and chapter plan using a deterministic local LLM stub.

## Getting Started

### 1. Python environment (Python 3.13)

```bash
python3.13 -m venv .venv
source .venv/bin/activate
pip install -e .
```

### 2. Run pipelines from the CLI

Once the environment is active you can interact with the sample pipeline using the bundled
`agent` command:

```bash
agent list
agent run book_outline \
  -p title="Voyagers" \
  -p genre="Sci-Fi" \
  -p audience="Adult"
```

Each run writes artifacts to `artifacts/run-<timestamp>/` and includes a `run.json` manifest plus any
persisted files generated by `store` steps.

## Repository Layout

```
core/          # pipeline orchestrator, prompt service, CLI
pipelines/     # JSON pipeline definitions
templates/     # Prompt templates rendered during llm_call steps
tools/         # Python transform helpers referenced by pipelines
artifacts/     # Run outputs (created at runtime)
```

## Next Steps

- Expand the orchestrator with branching/foreach semantics and richer artifact metadata.
- Replace the stub LLM client with pluggable backends (OpenAI, local ggml, etc.).
- Surface per-step outputs/logs in the CLI and allow rerunning from intermediate steps.
